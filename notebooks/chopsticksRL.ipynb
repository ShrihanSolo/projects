{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 18:59:34.287001: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameState():\n",
    "    \n",
    "    all_moves = ['alr', 'arl', 'arr', 'all', 's1', 's2', 's3', 's-1', 's-2', 's-3']\n",
    "    moves_dict = dict(zip(all_moves, np.arange(len(all_moves))))\n",
    "    \n",
    "    def __init__(self, playerleft = 1, playerright = 1, oppleft = 1, oppright = 1, turn = 0):\n",
    "        self.player = {\"l\": playerleft, \"r\": playerright}\n",
    "        self.opponent = {\"l\": oppleft, \"r\": oppright}\n",
    "        self.turn = turn # 0 for player, 1 for opponent\n",
    "        self.turn_number = 0\n",
    "    \n",
    "    def get_state(self):\n",
    "        return [self.player[\"l\"], self.player[\"r\"], self.opponent[\"l\"], self.opponent[\"r\"], self.turn, self.turn_number / 100]\n",
    "    \n",
    "    def __str__(self):\n",
    "        printleft = self.player[\"l\"]\n",
    "        printright = self.player[\"r\"]\n",
    "        printoppleft = self.opponent[\"l\"]\n",
    "        printoppright = self.opponent[\"r\"]\n",
    "        \n",
    "        \n",
    "        if printleft == 0:\n",
    "            printleft = \"~\"\n",
    "        \n",
    "        if printright == 0:\n",
    "            printright = \"~\"\n",
    "            \n",
    "        if printoppleft == 0:\n",
    "            printoppleft = \"~\"\n",
    "            \n",
    "        if printoppright == 0:\n",
    "            printoppright = \"~\"\n",
    "        \n",
    "        \n",
    "        return \"Player: \" + str(printleft) + \" \" + str(printright) + \" Opponent: \" + str(printoppleft) + \" \" + str(printoppright)\n",
    "    \n",
    "    def hand_order(self):\n",
    "        if self.player[\"l\"] > self.player[\"r\"]:\n",
    "            self.player[\"l\"], self.player[\"r\"] = self.player[\"r\"], self.player[\"l\"]\n",
    "        if self.opponent[\"l\"] > self.opponent[\"r\"]:\n",
    "            self.opponent[\"l\"], self.opponent[\"r\"] = self.opponent[\"r\"], self.opponent[\"l\"]\n",
    "    \n",
    "    def play_turn(self, string):\n",
    "        if string[0] == \"a\":\n",
    "            return self.play_attack_turn(string[1], string[2])\n",
    "        elif string[0] == \"s\":\n",
    "            return self.play_shift_turn(int(string[1:]))\n",
    "        elif string[0] == \"r\":\n",
    "            vmoves = self.get_valid_moves()\n",
    "            move = random.choice(vmoves)\n",
    "            print(\"Move:\", move)\n",
    "            return self.play_turn(move)\n",
    "    \n",
    "    def play_attack_turn(self, turnhand, receivehand):\n",
    "        \n",
    "        assert turnhand in [\"l\", \"r\"], \"Invalid hand\"\n",
    "        assert receivehand in [\"l\", \"r\"], \"Invalid hand\"\n",
    "        \n",
    "        \n",
    "        if self.turn == 0:\n",
    "            if self.player[turnhand] == 0:\n",
    "                print(\"Cannot attack with an empty hand!\")\n",
    "                return False\n",
    "\n",
    "            if self.opponent[receivehand] == 0:\n",
    "                print(\"Cannot hit an empty hand!\")\n",
    "                return False\n",
    "            \n",
    "            self.opponent[receivehand] = (self.player[turnhand] + self.opponent[receivehand]) % 5\n",
    "                    \n",
    "        elif self.turn == 1:\n",
    "            if self.opponent[turnhand] == 0:\n",
    "                print(\"Cannot attack with an empty hand!\")\n",
    "                return False\n",
    "\n",
    "            if self.player[receivehand] == 0:\n",
    "                print(\"Cannot hit an empty hand!\")\n",
    "                return False\n",
    "            \n",
    "            self.player[receivehand] = (self.player[receivehand] + self.opponent[turnhand]) % 5\n",
    "        \n",
    "        self.turn = 1 - self.turn\n",
    "        self.turn_number += 1\n",
    "        return True\n",
    "    \n",
    "    def play_shift_turn(self, torightcount):\n",
    "        \n",
    "        \n",
    "        if self.turn == 0: \n",
    "            tempr = self.player[\"r\"] + torightcount\n",
    "            templ = self.player[\"l\"] - torightcount\n",
    "            \n",
    "            if (tempr < 0) or (templ < 0) or (tempr >= 5) or (templ >= 5):\n",
    "                print(\"Invalid move!\")\n",
    "                return False\n",
    "            else:\n",
    "                self.player[\"r\"] = tempr\n",
    "                self.player[\"l\"] = templ\n",
    "            \n",
    "        elif self.turn == 1:\n",
    "            tempr = self.opponent[\"r\"] + torightcount\n",
    "            templ = self.opponent[\"l\"] - torightcount\n",
    "            \n",
    "            if (tempr < 0) or (templ < 0) or (tempr >= 5) or (templ >= 5):\n",
    "                print(\"Invalid move!\")\n",
    "                return False\n",
    "            else:\n",
    "                self.opponent[\"r\"] = tempr\n",
    "                self.opponent[\"l\"] = templ\n",
    "        \n",
    "        self.turn = 1 - self.turn\n",
    "        self.turn_number += 1\n",
    "        return True\n",
    "    \n",
    "    def is_game_over(self):\n",
    "        if self.player[\"l\"] == 0 and self.player[\"r\"] == 0:\n",
    "            return \"opponent\"\n",
    "        elif self.opponent[\"l\"] == 0 and self.opponent[\"r\"] == 0:\n",
    "            return \"player\"\n",
    "        elif self.turn_number >= 100:\n",
    "            return \"limit\"\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def get_valid_moves(self):\n",
    "        \n",
    "        vmoves = []\n",
    "\n",
    "        if self.turn == 0:\n",
    "            \n",
    "            if self.player[\"l\"] != 0:\n",
    "                if self.opponent[\"l\"] != 0:\n",
    "                    vmoves.append(\"all\")\n",
    "                if self.opponent[\"r\"] != 0:\n",
    "                    vmoves.append(\"alr\")\n",
    "                \n",
    "                for i in np.arange(1, self.player[\"l\"] + 1):\n",
    "                    if self.player[\"r\"] + i < 5:\n",
    "                        vmoves.append(\"s\" + str(i))\n",
    "                \n",
    "            if self.player[\"r\"] != 0:\n",
    "                if self.opponent[\"l\"] != 0:\n",
    "                    vmoves.append(\"arl\")\n",
    "                if self.opponent[\"r\"] != 0:\n",
    "                    vmoves.append(\"arr\")\n",
    "    \n",
    "                for i in range(1, self.player[\"r\"] + 1):\n",
    "                    if self.player[\"l\"] + i < 5:\n",
    "                        vmoves.append(\"s\" + str(-i))\n",
    "            \n",
    "            d = self.player[\"r\"] - self.player[\"l\"]\n",
    "            if d != 0:\n",
    "                vmoves.remove(\"s\" + str(-d))\n",
    "            \n",
    "        elif self.turn == 1:\n",
    "            if self.opponent[\"l\"] != 0:\n",
    "                if self.player[\"l\"] != 0:\n",
    "                    vmoves.append(\"all\")\n",
    "                if self.player[\"r\"] != 0:\n",
    "                    vmoves.append(\"alr\")\n",
    "                \n",
    "                for i in np.arange(1, self.opponent[\"l\"] + 1):\n",
    "                    if self.opponent[\"r\"] + i < 5:\n",
    "                        vmoves.append(\"s\" + str(i))\n",
    "                \n",
    "            if self.opponent[\"r\"] != 0:\n",
    "                if self.player[\"l\"] != 0:\n",
    "                    vmoves.append(\"arl\")\n",
    "                if self.player[\"r\"] != 0:\n",
    "                    vmoves.append(\"arr\")\n",
    "    \n",
    "                for i in range(1, self.opponent[\"r\"] + 1):\n",
    "                    if self.opponent[\"l\"] + i < 5:\n",
    "                        vmoves.append(\"s\" + str(-i))\n",
    "\n",
    "            d = self.opponent[\"r\"] - self.opponent[\"l\"]\n",
    "            if d != 0:\n",
    "                vmoves.remove(\"s\" + str(-d))\n",
    "            \n",
    "        return vmoves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent():\n",
    "    def reset(self):\n",
    "        pass\n",
    "    def reward(self, value):\n",
    "        pass\n",
    "    \n",
    "class RandomAgent(Agent):\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0\n",
    "    \n",
    "    def move(self, state):\n",
    "        return random.choice(state.get_valid_moves())\n",
    "    \n",
    "    def reward(self, value):\n",
    "        self.total_reward += value\n",
    "        return value\n",
    "\n",
    "class HumanAgent(Agent):\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0\n",
    "    \n",
    "    def ask_turn(self, gs):\n",
    "        print(\"Enter your move: \", end = '')\n",
    "        res = input()\n",
    "        print(res)\n",
    "        if res == \"p\":\n",
    "            print(gs)\n",
    "            return self.ask_turn(gs)\n",
    "        if res == \"q\":\n",
    "            return False\n",
    "        \n",
    "        vmoves = gs.get_valid_moves()\n",
    "        if res == \"g\":\n",
    "            print(\"Valid Moves:\", vmoves)\n",
    "            return self.ask_turn(gs)\n",
    "        if res not in vmoves:\n",
    "            print(\"Invalid move!\")\n",
    "            return self.ask_turn(gs)\n",
    "        return res\n",
    "    \n",
    "    def move(self, state):\n",
    "        res = self.ask_turn(state)\n",
    "        if res:\n",
    "            return res\n",
    "        else:\n",
    "            print(\"Quitting game!\")\n",
    "            return False\n",
    "    \n",
    "    def reward(self, value):\n",
    "        self.total_reward += value\n",
    "        return value\n",
    "\n",
    "class SmartAgent(Agent):\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0\n",
    "    \n",
    "    def move(self, state):\n",
    "        moves = state.get_valid_moves()\n",
    "        moves2 = moves.copy()\n",
    "        shiftmoves = [i for i in moves if i[0] == \"s\"]\n",
    "        \n",
    "        if state.turn == 0:\n",
    "            for h in [\"l\", \"r\"]:\n",
    "                for h2 in [\"l\", \"r\"]:\n",
    "                    if state.player[h] + state.opponent[h2] == 5:\n",
    "                        nonh = [\"l\", \"r\"]\n",
    "                        nonh.remove(h)\n",
    "                        nonh = nonh[0]\n",
    "                        if (state.player[h] + state.opponent[h] == 5) and (state.player[nonh] == 0):\n",
    "                            continue\n",
    "                        else:\n",
    "                            return \"a\" + h + h2\n",
    "                    \n",
    "            for sh in shiftmoves:\n",
    "                numsh = int(sh[1:])\n",
    "                \n",
    "                if numsh > 0:\n",
    "                    final_r = state.player[\"r\"] + numsh\n",
    "                    if (state.opponent[\"l\"] + final_r) % 5 == 0:\n",
    "                        moves2.remove(sh)\n",
    "                    elif (state.opponent[\"r\"] + final_r) % 5 == 0:\n",
    "                        moves2.remove(sh)\n",
    "                \n",
    "                if numsh < 0:\n",
    "                    final_l = state.player[\"l\"] - numsh\n",
    "                    if (state.opponent[\"l\"] + final_l) % 5 == 0:\n",
    "                        moves2.remove(sh)\n",
    "                    elif (state.opponent[\"r\"] + final_l) % 5 == 0:\n",
    "                        moves2.remove(sh)\n",
    "            \n",
    "            moves3 = moves2.copy()\n",
    "            \n",
    "            for h in [\"l\", \"r\"]:\n",
    "                for h2 in [\"l\", \"r\"]:\n",
    "                    if (2 * state.player[h] + state.opponent[h2]) % 5 == 0:\n",
    "                        if \"a\" + h + h2 in moves2:\n",
    "                            moves2.remove(\"a\" + h + h2)\n",
    "                if (state.player[\"r\"] + state.player[\"l\"] + state.opponent[h]) % 5 == 0:\n",
    "                    if \"al\" + h in moves2:\n",
    "                        moves2.remove(\"al\" + h)\n",
    "                    if \"ar\" + h in moves2:\n",
    "                        moves2.remove(\"ar\" + h)\n",
    "            \n",
    "            if len(moves2) == 0:\n",
    "                return random.choice(moves3)\n",
    "            \n",
    "            return random.choice(moves2)\n",
    "        \n",
    "        if state.turn == 1:\n",
    "            for h in [\"l\", \"r\"]:\n",
    "                for h2 in [\"l\", \"r\"]:\n",
    "                    if state.opponent[h] + state.player[h2] == 5:\n",
    "                        nonh = [\"l\", \"r\"]\n",
    "                        nonh.remove(h)\n",
    "                        nonh = nonh[0]\n",
    "                        if (state.opponent[h] + state.player[h] == 5) and (state.opponent[nonh] == 0):\n",
    "                            continue\n",
    "                        else:\n",
    "                            return \"a\" + h + h2\n",
    "                    \n",
    "            for sh in shiftmoves:\n",
    "                numsh = int(sh[1:])\n",
    "                \n",
    "                if numsh > 0:\n",
    "                    final_r = state.opponent[\"r\"] + numsh\n",
    "                    if (state.player[\"l\"] + final_r) % 5 == 0:\n",
    "                        moves2.remove(sh)\n",
    "                    elif (state.player[\"r\"] + final_r) % 5== 0:\n",
    "                        moves2.remove(sh)\n",
    "                \n",
    "                if numsh < 0:\n",
    "                    final_l = state.opponent[\"l\"] - numsh\n",
    "                    if (state.player[\"l\"] + final_l) % 5 == 0:\n",
    "                        moves2.remove(sh)\n",
    "                    elif (state.player[\"r\"] + final_l) % 5 == 0:\n",
    "                        moves2.remove(sh)\n",
    "            \n",
    "            moves3 = moves2.copy()\n",
    "            \n",
    "            for h in [\"l\", \"r\"]:\n",
    "                for h2 in [\"l\", \"r\"]:\n",
    "                    if (2 * state.opponent[h] + state.player[h2]) % 5 == 0:\n",
    "                        if \"a\" + h + h2 in moves2:\n",
    "                            moves2.remove(\"a\" + h + h2)\n",
    "                if (state.opponent[\"r\"] + state.opponent[\"l\"] + state.player[h]) % 5 == 0:\n",
    "                    if \"al\" + h in moves2:\n",
    "                        moves2.remove(\"al\" + h)\n",
    "                    if \"ar\" + h in moves2:\n",
    "                        moves2.remove(\"ar\" + h)\n",
    "            \n",
    "            if len(moves2) == 0:\n",
    "                return random.choice(moves3)\n",
    "            \n",
    "            return random.choice(moves2)\n",
    "    \n",
    "    def reward(self, value):\n",
    "        self.total_reward += value\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_single_game(state, playerA, playerB, verbose = False):\n",
    "    playerA.reset()\n",
    "    playerB.reset()\n",
    "    if verbose > 0:\n",
    "        print(state)\n",
    "        \n",
    "    while not state.is_game_over():\n",
    "        move = playerA.move(state)\n",
    "        if verbose > 0:\n",
    "            print(\"*\" * 20)\n",
    "            print(\"Player A move:\", move)\n",
    "        state.play_turn(move)\n",
    "        if verbose > 0:\n",
    "            print(state)\n",
    "        \n",
    "        goc = state.is_game_over()\n",
    "        if goc != 0:\n",
    "            if goc == \"limit\":\n",
    "                playerA.reward(0)\n",
    "                playerB.reward(0)\n",
    "                break\n",
    "            playerA.reward(1)\n",
    "            playerB.reward(-1)\n",
    "            break\n",
    "        \n",
    "        move = playerB.move(state)\n",
    "        if verbose > 0:\n",
    "            print(\"*\" * 20)\n",
    "            print(\"Player B move:\", move)\n",
    "        state.play_turn(move)\n",
    "        if verbose > 0:\n",
    "            print(state)\n",
    "        \n",
    "        goc = state.is_game_over()\n",
    "        if goc != 0:\n",
    "            if goc == \"limit\":\n",
    "                playerA.reward(0)\n",
    "                playerB.reward(0)\n",
    "                break\n",
    "            playerA.reward(-1)\n",
    "            playerB.reward(1)\n",
    "            break\n",
    "        \n",
    "    if verbose < 0:\n",
    "        print(playerA.q_history[-1])\n",
    "    return goc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateAgents(A, B, n = 1000, state = None, verbose = False):\n",
    "    if state is None:\n",
    "        state = GameState()\n",
    "    \n",
    "    Awin, Bwin, Draw = 0, 0, 0\n",
    "    for game_no in tqdm(range(n)):\n",
    "        winner = play_single_game(GameState(), A, B)\n",
    "        if winner == \"player\":\n",
    "            Awin += 1\n",
    "        elif winner == \"opponent\":\n",
    "            Bwin += 1\n",
    "        elif winner == \"limit\":\n",
    "            Draw += 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"*\" * 45)\n",
    "        aw = round(Awin / n * 100, 2)\n",
    "        bw = round(Bwin / n * 100, 2)\n",
    "        dw = round(Draw / n * 100, 2)\n",
    "        \n",
    "        print(f\"* P1 Win: {aw}% | P2 Win: {bw}% | Draw: {dw}% *\" )\n",
    "        print(\"*\" * 45)\n",
    "    \n",
    "    return Awin / n, Bwin / n, Draw / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randA = RandomAgent()\n",
    "randB = RandomAgent()\n",
    "\n",
    "evaluateAgents(randA, randB, 10000, verbose=True)\n",
    "evaluateAgents(randB, randA, 10000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agentA = SmartAgent()\n",
    "randB = RandomAgent()\n",
    "\n",
    "evaluateAgents(agentA, randB, 10000, verbose=True)\n",
    "evaluateAgents(randB, agentA, 10000, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agentA = SmartAgent()\n",
    "agentB = SmartAgent()\n",
    "\n",
    "evaluateAgents(agentA, agentB, 10000, verbose=True)\n",
    "evaluateAgents(agentB, agentA, 10000, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agentA = SmartAgent()\n",
    "randB = HumanAgent()\n",
    "\n",
    "play_single_game(GameState(), randB, agentA, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gs = GameState(2, 3, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent(Agent):\n",
    "    def __init__(self, training = True):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        #self.model.add(tf.keras.layers.Dense(6))\n",
    "        self.model.add(tf.keras.layers.Dense(len(GameState.all_moves)))\n",
    "        self.model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "        self.training = training\n",
    "        self.last_move = None\n",
    "        self.gamehistory = []\n",
    "        self.q_history = []\n",
    "    \n",
    "    def predict_q(self, state):\n",
    "        return self.model.predict(np.array([state.get_state()]), verbose = 0)\n",
    "\n",
    "    def fit_q(self, raw_state, q_values):\n",
    "        self.model.fit(raw_state, q_values, verbose = 0)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.last_move = None\n",
    "        self.gamehistory = []\n",
    "        self.q_history = []\n",
    "        \n",
    "    def move(self, state):\n",
    "        # always ask the agent to play the same side\n",
    "        q_values = self.predict_q(state)\n",
    "        temp_q = q_values.copy()\n",
    "        \n",
    "        invalid_move_idx = np.array(\n",
    "            [GameState.moves_dict[i] \n",
    "             for i in GameState.moves_dict \n",
    "             if i not in state.get_valid_moves()]\n",
    "        )\n",
    "        \n",
    "        temp_q[:, invalid_move_idx] = temp_q.min() - 1 # no illegal moves\n",
    "\n",
    "        move = GameState.all_moves[np.argmax(temp_q)]\n",
    "        value = temp_q.max()\n",
    "        \n",
    "        if self.training and self.last_move is not None:\n",
    "            self.reward(value)\n",
    "            \n",
    "        self.gamehistory.append(state.get_state().copy())\n",
    "        self.q_history.append(q_values) # Should probably be temp_q?\n",
    "        self.last_move = move\n",
    "        return move\n",
    "\n",
    "    \n",
    "    def reward(self, value):\n",
    "        if not self.training:\n",
    "            return\n",
    "        new_q = self.q_history[-1].copy()\n",
    "        new_q[:, GameState.moves_dict[self.last_move]] = value\n",
    "        self.fit_q(np.array([self.gamehistory[-1]]), new_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerA = RLAgent()\n",
    "playerB = RandomAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:03<00:38,  2.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m playerA\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluateAgents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayerA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayerB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m, in \u001b[0;36mevaluateAgents\u001b[0;34m(A, B, n, state, verbose)\u001b[0m\n\u001b[1;32m      5\u001b[0m Awin, Bwin, Draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m game_no \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n)):\n\u001b[0;32m----> 7\u001b[0m     winner \u001b[38;5;241m=\u001b[39m \u001b[43mplay_single_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGameState\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m winner \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplayer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      9\u001b[0m         Awin \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mplay_single_game\u001b[0;34m(state, playerA, playerB, verbose)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state\u001b[38;5;241m.\u001b[39mis_game_over():\n\u001b[0;32m----> 8\u001b[0m     move \u001b[38;5;241m=\u001b[39m \u001b[43mplayerA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m, in \u001b[0;36mRLAgent.move\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# always ask the agent to play the same side\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     temp_q \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     28\u001b[0m     invalid_move_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m     29\u001b[0m         [GameState\u001b[38;5;241m.\u001b[39mmoves_dict[i] \n\u001b[1;32m     30\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m GameState\u001b[38;5;241m.\u001b[39mmoves_dict \n\u001b[1;32m     31\u001b[0m          \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m state\u001b[38;5;241m.\u001b[39mget_valid_moves()]\n\u001b[1;32m     32\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mRLAgent.predict_q\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_q\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/RL/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/RL/lib/python3.11/site-packages/keras/engine/training.py:2378\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2376\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_begin()\n\u001b[1;32m   2377\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2378\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[1;32m   2379\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m   2380\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/RL/lib/python3.11/site-packages/keras/engine/data_adapter.py:1305\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1305\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset)\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[1;32m   1307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/RL/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:505\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    504\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/RL/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:713\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    709\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    711\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 713\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/RL/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:752\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    749\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    750\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[1;32m    751\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 752\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/RL/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3408\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3407\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3408\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3409\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3411\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "playerA.training = False\n",
    "evaluateAgents(playerA, playerB, n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 8132.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "* P1 Win: 52.0% | P2 Win: 48.0% | Draw: 0.0% *\n",
      "*********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.52, 0.48, 0.0)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluateAgents(RandomAgent(), RandomAgent(), n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/shri/opt/miniconda3/envs/RL/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/shri/opt/miniconda3/envs/RL/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/shri/opt/miniconda3/envs/RL/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/shri/opt/miniconda3/envs/RL/lib/python3.11/site-packages/keras/engine/training.py\", line 1052, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/Users/shri/opt/miniconda3/envs/RL/lib/python3.11/site-packages/keras/engine/training.py\", line 1016, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: No loss found. You may have forgotten to provide a `loss` argument in the `compile()` method.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m game_no \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)):\n\u001b[1;32m      3\u001b[0m     gs \u001b[38;5;241m=\u001b[39m GameState()\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mplay_single_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayerA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayerB\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m, in \u001b[0;36mplay_single_game\u001b[0;34m(state, playerA, playerB, verbose)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay_single_game\u001b[39m(state, playerA, playerB, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mplayerA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     playerB\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36mRLAgent.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamehistory\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_q_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_move \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamehistory \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mRLAgent.fit_q\u001b[0;34m(self, raw_state, q_values)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_q\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_state, q_values):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/RL/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/h0/qsvw48hn5m5bx13015zw15dc0000gn/T/__autograph_generated_filepei0v_8y.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/shri/opt/miniconda3/envs/RL/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/shri/opt/miniconda3/envs/RL/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/shri/opt/miniconda3/envs/RL/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/shri/opt/miniconda3/envs/RL/lib/python3.11/site-packages/keras/engine/training.py\", line 1052, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/Users/shri/opt/miniconda3/envs/RL/lib/python3.11/site-packages/keras/engine/training.py\", line 1016, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: No loss found. You may have forgotten to provide a `loss` argument in the `compile()` method.\n"
     ]
    }
   ],
   "source": [
    "playerA.training = True\n",
    "for game_no in tqdm(range(100)):\n",
    "    gs = GameState()\n",
    "    play_single_game(gs, playerA, playerB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:05<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "* P1 Win: 58.0% | P2 Win: 42.0% | Draw: 0.0% *\n",
      "*********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.58, 0.42, 0.0)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playerA.training = False\n",
    "evaluateAgents(playerA, playerB, n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:03<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "playerA.training = True\n",
    "for game_no in tqdm(range(100)):\n",
    "    gs = GameState()\n",
    "    play_single_game(gs, playerA, playerB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:55<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "* P1 Win: 69.0% | P2 Win: 31.0% | Draw: 0.0% *\n",
      "*********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.69, 0.31, 0.0)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playerA.training = False\n",
    "evaluateAgents(playerA, playerB, n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:19<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "playerA.training = True\n",
    "for game_no in tqdm(range(100)):\n",
    "    gs = GameState()\n",
    "    play_single_game(gs, playerA, playerB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:03<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "* P1 Win: 75.0% | P2 Win: 24.0% | Draw: 1.0% *\n",
      "*********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.75, 0.24, 0.01)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playerA.training = False\n",
    "evaluateAgents(playerA, playerB, n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:52<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "playerA.training = True\n",
    "for game_no in tqdm(range(100)):\n",
    "    gs = GameState()\n",
    "    play_single_game(gs, playerA, playerB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:05<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "* P1 Win: 76.0% | P2 Win: 23.0% | Draw: 1.0% *\n",
      "*********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.76, 0.23, 0.01)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playerA.training = False\n",
    "evaluateAgents(playerA, playerB, n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:04<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "playerA.training = True\n",
    "for game_no in tqdm(range(100)):\n",
    "    gs = GameState()\n",
    "    play_single_game(gs, playerA, playerB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:06<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "* P1 Win: 81.0% | P2 Win: 19.0% | Draw: 0.0% *\n",
      "*********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.81, 0.19, 0.0)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playerA.training = False\n",
    "evaluateAgents(playerA, playerB, n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:01<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "playerA.training = True\n",
    "for game_no in tqdm(range(100)):\n",
    "    gs = GameState()\n",
    "    play_single_game(gs, playerA, playerB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:59<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "* P1 Win: 87.0% | P2 Win: 12.0% | Draw: 1.0% *\n",
      "*********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.87, 0.12, 0.01)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playerA.training = False\n",
    "evaluateAgents(playerA, playerB, n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:55<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "playerA.training = True\n",
    "for game_no in tqdm(range(100)):\n",
    "    gs = GameState()\n",
    "    play_single_game(gs, playerA, playerB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:49<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "* P1 Win: 82.0% | P2 Win: 18.0% | Draw: 0.0% *\n",
      "*********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.82, 0.18, 0.0)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playerA.training = False\n",
    "evaluateAgents(playerA, playerB, n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerRL = RLAgent()\n",
    "playerRL.model = keras.models.load_model(\"RLAgentBestModel.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [09:46<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "playerB = RandomAgent()\n",
    "playerRL.training = True\n",
    "for game_no in tqdm(range(500)):\n",
    "    gs = GameState()\n",
    "    play_single_game(gs, playerRL, playerB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:47<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "* P1 Win: 79.0% | P2 Win: 21.0% | Draw: 0.0% *\n",
      "*********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.79, 0.21, 0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playerRL.training = False\n",
    "evaluateAgents(playerRL, playerB, n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerRLOrig = RLAgent()\n",
    "playerRLOrig.model = keras.models.load_model(\"RLAgentBestModel.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:56<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "* P1 Win: 1.0% | P2 Win: 98.0% | Draw: 1.0% *\n",
      "*********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.01, 0.98, 0.01)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smartPlayer = SmartAgent()\n",
    "playerRL.training = False\n",
    "evaluateAgents(playerRL, smartPlayer, n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerRL.model.save(\"RLAgentBestModel1700.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:51<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "playerRL = RLAgent()\n",
    "playerSA = SmartAgent()\n",
    "\n",
    "playerRL.training = True\n",
    "for game_no in tqdm(range(500)):\n",
    "    \n",
    "    gs = GameState(*np.random.randint(1, 5, 4))\n",
    "    play_single_game(gs, playerRL, playerSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "* P1 Win: 0.0% | P2 Win: 100.0% | Draw: 0.0% *\n",
      "*********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playerRL.training = False\n",
    "evaluateAgents(playerRL, playerSA, n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [06:00<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "playerRL.training = True\n",
    "for game_no in tqdm(range(500)):\n",
    "    \n",
    "    gs = GameState(*np.random.randint(1, 5, 4))\n",
    "    play_single_game(gs, playerRL, playerSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:29<00:00,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "* P1 Win: 1.0% | P2 Win: 99.0% | Draw: 0.0% *\n",
      "*********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.01, 0.99, 0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playerRL.training = False\n",
    "evaluateAgents(playerRL, playerSA, n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerRL.model.save(\"RLSmartAgent1000.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [07:41<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "playerRL.training = True\n",
    "for game_no in tqdm(range(500)):\n",
    "    \n",
    "    gs = GameState(*np.random.randint(1, 5, 4))\n",
    "    play_single_game(gs, playerRL, playerSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:35<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "* P1 Win: 1.0% | P2 Win: 99.0% | Draw: 0.0% *\n",
      "*********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.01, 0.99, 0.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playerRL.training = False\n",
    "evaluateAgents(playerRL, playerSA, n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [07:56<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "playerRL.training = True\n",
    "for game_no in tqdm(range(500)):\n",
    "    \n",
    "    gs = GameState(*np.random.randint(1, 5, 4))\n",
    "    play_single_game(gs, playerRL, playerSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************************\n",
      "* P1 Win: 0.0% | P2 Win: 100.0% | Draw: 0.0% *\n",
      "*********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playerRL.training = False\n",
    "evaluateAgents(playerRL, playerSA, n = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerRL.model.save(\"RLSmartAgent2000.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
